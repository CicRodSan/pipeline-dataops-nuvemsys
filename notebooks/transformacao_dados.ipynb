{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9012a718",
   "metadata": {},
   "source": [
    "# Como criar um cluster Databricks para este notebook\\n\n",
    "\\n\n",
    "1. No menu lateral do Databricks, clique em **Compute** (ou **Clusters**).\\n\n",
    "2. Clique em **Create Cluster**.\\n\n",
    "3. Preencha as configurações:\\n\n",
    "   - **Cluster name:** cluster-transformacao\\n\n",
    "   - **Cluster mode:** Standard\\n\n",
    "   - **Databricks Runtime Version:** 10.4 LTS (ou superior)\\n\n",
    "   - **Node type:** Standard_DS3_v2 (ou equivalente)\\n\n",
    "   - **Number of workers:** 1 ou 2 para testes\\n\n",
    "   - **Auto Termination:** 30 minutos (opcional)\\n\n",
    "4. Clique em **Create Cluster**.\\n\n",
    "5. Quando o cluster estiver ativo, selecione-o no topo deste notebook para executar as células."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a971a9",
   "metadata": {},
   "source": [
    "## Configuração da Sessão Spark\\n\n",
    "Inicializa a sessão Spark. A configuração exata pode precisar de ajustes dependendo do ambiente (Databricks, Synapse)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedc34da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializa a sessão Spark (Databricks)\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TransformacaoDados\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"SparkSession criada com sucesso: {spark.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f46f6cb",
   "metadata": {},
   "source": [
    "## Leitura dos Dados Ingeridos\\n\n",
    "Lê os arquivos Parquet do NOAA ISD que foram copiados pelo pipeline de ingestão para o contêiner 'output'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe78a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leitura dos Dados Ingeridos\n",
    "storage_account_name = \"datavalidation456\"  # Nome real da sua storage account\n",
    "container_name = \"output\"  # Nome real do container\n",
    "\n",
    "# Caminho dos dados Parquet\n",
    "input_path = f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/\"\n",
    "\n",
    "print(f\"Lendo dados de: {input_path}\")\n",
    "\n",
    "df_raw = spark.read.parquet(input_path)\n",
    "print(\"Schema dos dados lidos:\")\n",
    "df_raw.printSchema()\n",
    "df_raw.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69060617",
   "metadata": {},
   "source": [
    "## Exploração e Inspeção de Dados\n",
    "Vamos examinar os dados para entender melhor sua estrutura e conteúdo antes de realizar transformações."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa334658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploração básica\n",
    "print(f\"Contagem total de registros: {df_raw.count()}\")\n",
    "\n",
    "# Ver distribuição por ano\n",
    "from pyspark.sql.functions import year, col, count\n",
    "\n",
    "# Assumindo que existe uma coluna com timestamp\n",
    "# Ajuste o nome da coluna conforme necessário\n",
    "if 'date' in df_raw.columns:\n",
    "    timestamp_col = 'date'\n",
    "elif 'datetime' in df_raw.columns:\n",
    "    timestamp_col = 'datetime'\n",
    "elif 'timestamp' in df_raw.columns:\n",
    "    timestamp_col = 'timestamp'\n",
    "else:\n",
    "    # Imprimir colunas disponíveis\n",
    "    print(\"Colunas disponíveis:\")\n",
    "    print(df_raw.columns)\n",
    "    timestamp_col = None\n",
    "\n",
    "if timestamp_col:\n",
    "    print(\"\\nDistribuição de registros por ano:\")\n",
    "    df_raw.groupBy(year(col(timestamp_col)).alias(\"ano\")) \\\n",
    "        .agg(count(\"*\").alias(\"contagem\")) \\\n",
    "        .orderBy(\"ano\") \\\n",
    "        .show(20)\n",
    "\n",
    "# Verificar valores nulos\n",
    "print(\"\\nContagem de valores nulos por coluna:\")\n",
    "from pyspark.sql.functions import col, count, isnan, when\n",
    "\n",
    "def null_value_count(df):\n",
    "    return df.select([count(when(col(c).isNull() | isnan(col(c)), c)).alias(c) for c in df.columns])\n",
    "\n",
    "null_value_count(df_raw).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc3bc11",
   "metadata": {},
   "source": [
    "## Transformação de Dados\n",
    "Vamos realizar as seguintes transformações nos dados meteorológicos:\n",
    "1. Filtrar apenas o ano mais recente (2023)\n",
    "2. Selecionar e renomear colunas relevantes\n",
    "3. Eliminar registros com valores nulos em campos importantes\n",
    "4. Converter unidades de medida quando necessário"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a69ebeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificar o esquema e colunas necessárias\n",
    "# Isso ajudará na adaptação do código para o esquema real\n",
    "import re\n",
    "from pyspark.sql.functions import col, year, month, dayofmonth, hour, expr, to_timestamp, lit\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# Mapeamento para colunas esperadas - ajuste conforme seu esquema real\n",
    "# Este é um mapeamento genérico baseado em dados meteorológicos típicos\n",
    "column_mappings = {\n",
    "    # Identificadores de estação (USAF/WBAN são comuns no formato NOAA ISD)\n",
    "    r'^(usaf|station_id)$': 'station_id_usaf',\n",
    "    r'^(wban)$': 'station_id_wban',\n",
    "    \n",
    "    # Coordenadas geográficas\n",
    "    r'^(lat|latitude)$': 'latitude',\n",
    "    r'^(lon|long|longitude)$': 'longitude',\n",
    "    r'^(elev|elevation)$': 'elevation',\n",
    "    \n",
    "    # Data e hora\n",
    "    r'^(date|datetime|time|timestamp)$': 'timestamp',\n",
    "    \n",
    "    # Temperaturas - geralmente em diferentes formatos\n",
    "    r'^(temp|temperature|air_temperature|t_mean)$': 'temperature'\n",
    "}\n",
    "\n",
    "# Mapear as colunas existentes para as colunas padronizadas\n",
    "mapped_columns = {}\n",
    "for col_name in df_raw.columns:\n",
    "    for pattern, mapped_name in column_mappings.items():\n",
    "        if re.match(pattern, col_name.lower()):\n",
    "            mapped_columns[col_name] = mapped_name\n",
    "            break\n",
    "\n",
    "print(\"Mapeamento de colunas detectado:\")\n",
    "for orig, mapped in mapped_columns.items():\n",
    "    print(f\"{orig} -> {mapped}\")\n",
    "\n",
    "# Filtrar ano mais recente (2023)\n",
    "if 'timestamp' in mapped_columns.values():\n",
    "    orig_timestamp_col = [k for k, v in mapped_columns.items() if v == 'timestamp'][0]\n",
    "    \n",
    "    # Criar DataFrame com colunas renomeadas\n",
    "    df_renamed = df_raw.select(\n",
    "        *[col(orig_col).alias(mapped_col) \n",
    "          for orig_col, mapped_col in mapped_columns.items()]\n",
    "    )\n",
    "    \n",
    "    # Filtrar por ano (2023)\n",
    "    df_filtered = df_renamed.filter(year(col(\"timestamp\")) == 2023)\n",
    "    \n",
    "    print(f\"\\nRegistros após filtrar por ano 2023: {df_filtered.count()}\")\n",
    "else:\n",
    "    print(\"\\nColuna de timestamp não identificada. Pulando filtro por ano.\")\n",
    "    df_filtered = df_raw\n",
    "\n",
    "# Remover registros com valores nulos em colunas críticas\n",
    "critical_columns = [col_name for col_name in [\n",
    "    'station_id_usaf', 'timestamp', 'temperature'\n",
    "] if col_name in df_filtered.columns]\n",
    "\n",
    "df_clean = df_filtered\n",
    "for col_name in critical_columns:\n",
    "    df_clean = df_clean.filter(col(col_name).isNotNull())\n",
    "\n",
    "print(f\"\\nRegistros após remover nulos em colunas críticas: {df_clean.count()}\")\n",
    "\n",
    "# Conversão de unidades (se necessário)\n",
    "# Exemplo: converter temperatura de Fahrenheit para Celsius se necessário\n",
    "# Verificando a faixa de valores para determinar a unidade\n",
    "if 'temperature' in df_clean.columns:\n",
    "    temp_stats = df_clean.select(\n",
    "        expr(\"min(temperature)\").alias(\"min_temp\"),\n",
    "        expr(\"max(temperature)\").alias(\"max_temp\"),\n",
    "        expr(\"avg(temperature)\").alias(\"avg_temp\")\n",
    "    ).collect()[0]\n",
    "    \n",
    "    print(f\"\\nEstatísticas de temperatura:\")\n",
    "    print(f\"Mínima: {temp_stats['min_temp']}\")\n",
    "    print(f\"Máxima: {temp_stats['max_temp']}\")\n",
    "    print(f\"Média: {temp_stats['avg_temp']}\")\n",
    "    \n",
    "    # Se a temperatura máxima for muito alta, provavelmente está em Fahrenheit\n",
    "    if temp_stats['max_temp'] > 50:  # assumimos que temperaturas acima de 50 são Fahrenheit\n",
    "        print(\"\\nConvertendo temperaturas de Fahrenheit para Celsius...\")\n",
    "        df_clean = df_clean.withColumn(\n",
    "            \"temperature\", \n",
    "            ((col(\"temperature\") - 32) * 5/9).cast(DoubleType())\n",
    "        )\n",
    "        \n",
    "        # Verificar novas estatísticas após conversão\n",
    "        new_temp_stats = df_clean.select(\n",
    "            expr(\"min(temperature)\").alias(\"min_temp\"),\n",
    "            expr(\"max(temperature)\").alias(\"max_temp\"),\n",
    "            expr(\"avg(temperature)\").alias(\"avg_temp\")\n",
    "        ).collect()[0]\n",
    "        \n",
    "        print(f\"\\nEstatísticas após conversão:\")\n",
    "        print(f\"Mínima: {new_temp_stats['min_temp']}\")\n",
    "        print(f\"Máxima: {new_temp_stats['max_temp']}\")\n",
    "        print(f\"Média: {new_temp_stats['avg_temp']}\")\n",
    "\n",
    "# Visualizar resultado da transformação\n",
    "print(\"\\nEsquema dos dados transformados:\")\n",
    "df_clean.printSchema()\n",
    "print(\"\\nAmostra dos dados transformados:\")\n",
    "df_clean.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f3f713",
   "metadata": {},
   "source": [
    "## Gravação dos Dados Transformados\n",
    "Vamos salvar os dados transformados em formato Parquet no Blob Storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c9d681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurações para salvar os dados transformados\n",
    "# Usar o mesmo storage account, mas um diretório específico para dados transformados\n",
    "transformed_folder = \"transformed_weather_data\"\n",
    "output_path = f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/{transformed_folder}/\"\n",
    "\n",
    "print(f\"Salvando dados transformados em: {output_path}\")\n",
    "\n",
    "# Escrever os dados em formato Parquet\n",
    "# Modo 'overwrite' substituirá dados existentes\n",
    "try:\n",
    "    df_clean.write \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .parquet(output_path)\n",
    "    print(\"\\nDados transformados gravados com sucesso!\")\n",
    "    print(f\"Total de registros gravados: {df_clean.count()}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nErro ao gravar dados transformados: {str(e)}\")\n",
    "\n",
    "# Confirmar se os dados foram gravados corretamente lendo-os de volta\n",
    "try:\n",
    "    df_verification = spark.read.parquet(output_path)\n",
    "    print(\"\\nVerificação da gravação:\")\n",
    "    print(f\"Contagem de registros lidos: {df_verification.count()}\")\n",
    "    df_verification.show(5)\n",
    "except Exception as e:\n",
    "    print(f\"\\nErro ao ler dados transformados para verificação: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7e87ee",
   "metadata": {},
   "source": [
    "## Resumo e Métricas de Qualidade\n",
    "Vamos calcular algumas métricas para avaliar a qualidade dos dados transformados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7b1fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular métricas de qualidade dos dados\n",
    "print(\"Métricas de qualidade dos dados transformados:\")\n",
    "\n",
    "# Contagem total de registros\n",
    "total_records = df_clean.count()\n",
    "print(f\"\\nTotal de registros: {total_records}\")\n",
    "\n",
    "# Verificar completude dos dados (ausência de valores nulos)\n",
    "from pyspark.sql.functions import col, count, when, isnan, round as spark_round\n",
    "\n",
    "# Calcular porcentagem de valores não-nulos para cada coluna\n",
    "completeness = {}\n",
    "for column_name in df_clean.columns:\n",
    "    non_null_count = df_clean.filter(~(col(column_name).isNull() | isnan(col(column_name)))).count()\n",
    "    completeness[column_name] = (non_null_count / total_records) * 100\n",
    "\n",
    "print(\"\\nCompletude dos dados (% de valores não-nulos):\")\n",
    "for col_name, percentage in completeness.items():\n",
    "    print(f\"{col_name}: {percentage:.2f}%\")\n",
    "\n",
    "# Estatísticas para colunas numéricas\n",
    "numeric_columns = [column_name for column_name in df_clean.columns\n",
    "                 if df_clean.schema[column_name].dataType.typeName() in [\"double\", \"integer\", \"long\", \"float\"]]\n",
    "\n",
    "if numeric_columns:\n",
    "    print(\"\\nEstatísticas para colunas numéricas:\")\n",
    "    df_clean.select([spark_round(expr(f\"min({col})\"), 2).alias(f\"min_{col}\") for col in numeric_columns] +\n",
    "                   [spark_round(expr(f\"max({col})\"), 2).alias(f\"max_{col}\") for col in numeric_columns] +\n",
    "                   [spark_round(expr(f\"avg({col})\"), 2).alias(f\"avg_{col}\") for col in numeric_columns]).show()\n",
    "\n",
    "# Verificar distribuição por meses (se tiver timestamp)\n",
    "if 'timestamp' in df_clean.columns:\n",
    "    print(\"\\nDistribuição de registros por mês em 2023:\")\n",
    "    df_clean.groupBy(month(col(\"timestamp\")).alias(\"mês\"))\\\n",
    "        .agg(count(\"*\").alias(\"contagem\"))\\\n",
    "        .orderBy(\"mês\")\\\n",
    "        .show()\n",
    "\n",
    "print(\"\\nProcessamento de dados concluído!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
